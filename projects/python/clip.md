Currently, CLIP (Contrastive Languageâ€“Image Pre-training) - a SOTA Visual Language Model - by OpenAI enables users to map between images and natural language. Howewer, to make zero-shot predictions, CLIP users have to go through a manual process of implementing, otherwise boilerplate, functionality to get a pre-trained CLIP instance up and running. As part of this project, I would like to implement a Python library that will provide generic functionality through single function calls for zero-shot image classification as well as computing alignment between a sequence of natural language annotations and images. 

While this would be very handy for CLIP users, I would like to further enable this project to contribute to my research, which (in a nutshell) utilizes CLIP to predict the current state of a text progression, visually and grounded by natural language. 

For reference, CLIP by OpenAI: [link](https://openai.com/blog/clip/)
