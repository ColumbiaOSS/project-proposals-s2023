# Open Source Development

Name: Yash Datta

UNI: yd2590

Github: [link](https://github.com/saucam)

## Project Proposal

Recently, I got interested in indexing libraries for similarity search. These are highly efficient libraries that index vector data and can be used to query 'similar' vector based on metrics like euclidean distance etc.

Some of the very popular implementations are:

- [Faiss](https://github.com/facebookresearch/faiss)
- [hnswlib](https://github.com/nmslib/hnswlib)

Most of these libraries have C++ implementations with python bindings, but there is general [interest in the community for JVM implementations](https://github.com/facebookresearch/faiss/issues/105) as well.

I was able to find a java implementation of [hnswlib here](https://github.com/jelmerk/hnswlib).
The major limitations of such libraries is memory. They rely on storing all the indexed data in-memory. So they need to be very memory efficient. On the other hand, they need to be really fast to handle queries in reasonable time.

I have been working professionally with scala for 6+ years (out of 13+ years) as a developer.
I would like to roll my own scala implementation. Here is what I would do differently:

- Concentrate on a simple application level use-case of finding similar items, and do away with other constraints and additional features that these implementations support. In particular, the java implementation above guarantees thread safety and therefore has to use atomic references bringing in additional overhead while storing the objects in memory.
- Simple applications would not require thread safety of index structures and if they would, they could handle it on the application layer itself.
- Take a hit (within reason) on performance to optimize memory.
- I want to experiment with and use faster primitive data libraries like [hppc](https://labs.carrotsearch.com/hppc.html), [fastutil](https://fastutil.di.unimi.it/), [koloboke](https://github.com/leventov/Koloboke) etc for storing the vectors.
- Experiment with storing data off-heap, to further reduce memory requirements.
- Eventually I would like to support many different indexes since each has their own pros and cons.
- Experiment with possibilty to use data structures that spill to disk? There is a newer [alternate hybrid algorithm](https://openreview.net/forum?id=-1rrzmJCp4) that claims to support data both in disk and in memory but its performance with different kinds of data sets is yet to be proven.

## Project Contributions

As I am interested in semantic search, I was exploring projects using the indexing projects mentioned above. Therefore, I would like to contribute to the below project, that internally supports all the major indexing schemes mentioned above.

- [txtai](https://github.com/neuml/txtai): txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. It is written purely in python.

Some other projects of interest:

- [scikit-learn](https://github.com/scikit-learn/scikit-learn)
- [pytorch](https://github.com/pytorch/pytorch)

As I have previously worked with parquet data format and contributed some changes related  to that in apache-spark project, I think I can also contribute to the below project

- [parquet4s](https://github.com/mjakubowski84/parquet4s): Parquet4s is a simple I/O for Parquet. Allows you to easily read and write Parquet files in Scala.
