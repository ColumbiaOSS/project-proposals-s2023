# Open Source Development

Name: Ege Ozguroglu

UNI: eo2464

Github: [link](https://github.com/egeozguroglu)

## Project Proposal
CLIP: [link](https://openai.com/blog/clip/)

Currently, CLIP (Contrastive Languageâ€“Image Pre-training) - a SOTA Visual Language Model - by OpenAI enables users to map between images and natural language. Howewer, to make zero-shot predictions, CLIP users have to go through a manual process of implementing, otherwise boilerplate, functionality to get a pre-trained CLIP instance up and running. As part of this project, I would like to implement a Python library that will provide generic functionality through single function calls for zero-shot image classification as well as computing alignment between a sequence of natural language annotations and images. 

While this would be very handy for CLIP users, I would like to further enable this project to contribute to my research, which (in a nutshell) utilizes CLIP to predict the current state of a text progression, visually and grounded by natural language. 

## Project Contributions

(1) To start off, as a first contribution, I would like to contribute to numpy's good first issues (currently > 2k): https://github.com/numpy/numpy/issues

(2) Secondly, at medium-level difficulty, I would like to contribute to PyTorch, which I'm using on a daily basis: https://github.com/pytorch/pytorch/issues

(3) CLIP, with which I'm interacting intensively for my research, has > 100 issues currently: https://github.com/openai/CLIP/issues While my familiarity will help, tackling these issues will be on the harder side. 
